{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497aab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sqlalchemy pyodbc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33962237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil, os\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "products = [\n",
    "    { \"id\": 1, \"pname\": \"Laptop\", \"price\": 12000 },\n",
    "    { \"id\": 2, \"pname\": \"Mouse\", \"price\": 250 },\n",
    "    { \"id\": 3, \"pname\": \"Keyboard\", \"price\": 500 },\n",
    "    { \"id\": 4, \"pname\": \"Monitor\", \"price\": 3000 },\n",
    "    { \"id\": 5, \"pname\": \"Headphones\", \"price\": 800 },\n",
    "    { \"id\": 6, \"pname\": \"Printer\", \"price\": 4500 },\n",
    "    { \"id\": 7, \"pname\": \"USB Flash Drive\", \"price\": 150 }\n",
    "]\n",
    "with open(\"products.json\", \"w\") as file:\n",
    "    json.dump(products, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d1e91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_transaction_data_generator(number_of_records,max_id):\n",
    "    return [\n",
    "            [\n",
    "                i,\n",
    "                random.randint(1,2001),\n",
    "                random.randint(1, 7),\n",
    "                random.randint(1, 10),\n",
    "                date.today(),\n",
    "                random.choice(['cairo', 'alex', 'tanta'])\n",
    "            ]\n",
    "            for i in range(max_id+1, max_id + number_of_records + 1)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a02e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\",\"transaction_date\", \"branch\"]\n",
    "\n",
    "def create_transactions_dfs(num_of_files, number_of_records):\n",
    "    base_date = fake.date_between(\"-15d\",\"-4d\")\n",
    "    file_counter = 1\n",
    "    current_id = 0\n",
    "    \n",
    "    for i in range(num_of_files):\n",
    "        current_date = base_date + timedelta(days=i // 3)\n",
    "        \n",
    "        transaction_df = pd.DataFrame(\n",
    "            columns=columns_names,\n",
    "            data= new_transaction_data_generator(number_of_records,current_id)\n",
    "        )\n",
    "        \n",
    "        transaction_df.to_csv(f\"transactions{file_counter}.csv\", index=False)\n",
    "        file_counter += 1\n",
    "        current_id += number_of_records\n",
    "\n",
    "create_transactions_dfs(num_of_files=12, number_of_records=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_id(file):\n",
    "    if not os.path.exists(file):\n",
    "        return 0\n",
    "    existing_df = pd.read_csv(file)\n",
    "    return len(existing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e37d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = \"transactions.csv\"\n",
    "def concat_files(sec_file):\n",
    "    # If the main file does not exist, just copy secondary to main\n",
    "    if not os.path.exists(main_file):\n",
    "        shutil.copy(sec_file, main_file)\n",
    "        return\n",
    "    \n",
    "    # Read \n",
    "    main_df = pd.read_csv(main_file)\n",
    "    # skip el line bta3 el header w ent bt read\n",
    "    sec_df = pd.read_csv(sec_file,skiprows=1,names=columns_names)\n",
    "\n",
    "    # Append directly (IDs already unique)\n",
    "    df_final = pd.concat([main_df, sec_df], ignore_index=True)\n",
    "    df_final.to_csv(main_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 13):\n",
    "    sec_file = f\"transactions{i}.csv\"\n",
    "    concat_files(sec_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d40768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_records(num_of_customers):\n",
    "    Ccolumn_names=['id','first_name','last_name','Caddress']\n",
    "    customers_df = pd.DataFrame(columns=Ccolumn_names,\n",
    "                                data =[[i,\n",
    "                                fake.first_name(),\n",
    "                                fake.last_name() ,\n",
    "                                fake.address().replace('\\n','')]\n",
    "                                for i in range(1, num_of_customers+1)] )    \n",
    "    \n",
    "    customers_df.to_csv('Customers.csv',index=False)\n",
    "create_customer_records(2000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c688e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df = pd.read_csv('transactions.csv')\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "products_df = pd.read_json('products.json')\n",
    "orders_df = pd.read_csv('Orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_orders(transaction_df,customers_df,products_df):\n",
    "    merged_df = pd.merge(\n",
    "        transaction_df, customers_df,\n",
    "        left_on='customer_id', right_on='id',\n",
    "        how='inner', suffixes=('', '_cust')\n",
    "    )\n",
    "\n",
    "    \n",
    "    orders_df = pd.merge(\n",
    "        merged_df, products_df,\n",
    "        left_on='product_id', right_on='id',\n",
    "        how='inner', suffixes=('', '_prod')\n",
    "    )\n",
    "\n",
    "    orders_df['amount'] = (orders_df['price'] * orders_df['quantity']).round().astype('int64')\n",
    "    orders_df['customer_full_name'] = orders_df['first_name'] + ' ' + orders_df['last_name']\n",
    "\n",
    "    orders_df = orders_df[\n",
    "        ['transaction_id', 'pname', 'amount', 'customer_full_name',\n",
    "        'transaction_date', 'branch']\n",
    "    ].rename(columns={'pname': 'product_name'})\n",
    "\n",
    "    orders_df.to_csv('Orders.csv', index=False)\n",
    "\n",
    "    return orders_df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_into_dfs(file):\n",
    "    return pd.read_csv(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://@DESKTOP-1UV6SC4\\\\SQLEXPRESS/ETLTask\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes\"\n",
    "    \"&Encrypt=no\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e010487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into_ssms(engine):\n",
    "    orders_df.to_sql(\"TOrders\" , con=engine , if_exists=\"append\" , index= False)\n",
    "    customers_df.to_sql(\"Tcustomers\" , con=engine , if_exists=\"append\" , index= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7115bf6",
   "metadata": {},
   "source": [
    " # * Insert additional records in transactions.csv\n",
    " # * Insert only recirds that doesn't already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0428937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transactions_toCSV(number_of_records, file_path=\"transactions.csv\"):\n",
    "    columns_names = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\",\"transaction_date\", \"branch\"]\n",
    "    if os.path.exists(file_path):\n",
    "        existing_df = pd.read_csv(file_path, usecols=[\"transaction_id\"])\n",
    "        max_id = existing_df[\"transaction_id\"].max()\n",
    "        write_header = False\n",
    "        \n",
    "    else:\n",
    "        max_id = 0\n",
    "        write_header = True\n",
    "\n",
    "    new_data = new_transaction_data_generator(number_of_records,max_id)\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=columns_names)\n",
    "\n",
    "    new_df.to_csv(file_path, index=False, header=write_header, mode='a')\n",
    "add_transactions_toCSV(500)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e475b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 500 new transactions.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect, text\n",
    "def insert_into_orders_db(file, engine):\n",
    "    transaction_df = read_csv_into_dfs(file)\n",
    "\n",
    "    inspector = inspect(engine)\n",
    "    table_exists = inspector.has_table(\"TOrders\")\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create table\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE TOrders (\n",
    "            transaction_id INT PRIMARY KEY,\n",
    "            product_name NVARCHAR(100),\n",
    "            amount INT,\n",
    "            customer_full_name NVARCHAR(200),\n",
    "            transaction_date DATE,\n",
    "            branch NVARCHAR(100)\n",
    "        );\n",
    "        \"\"\"\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "\n",
    "        # Merge & insert all transactions\n",
    "        orders_df = merge_orders(transaction_df, customers_df, products_df)\n",
    "        orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    else:\n",
    "        with engine.connect() as conn:\n",
    "            row_count = conn.execute(text(\"SELECT COUNT(*) FROM TOrders\")).scalar()\n",
    "\n",
    "        if row_count == 0:\n",
    "            orders_df = merge_orders(transaction_df, customers_df, products_df)\n",
    "            orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "        else:\n",
    "            # Get max transaction_id already in DB\n",
    "            max_id_df = pd.read_sql(\"SELECT MAX(transaction_id) AS max_id FROM TOrders\", con=engine)\n",
    "            max_id = max_id_df.iloc[0, 0]\n",
    "\n",
    "            # Filter only new transactions\n",
    "            transaction_new = transaction_df[transaction_df[\"transaction_id\"] > max_id]\n",
    "\n",
    "            if not transaction_new.empty:\n",
    "                orders_df = merge_orders(transaction_new, customers_df, products_df)\n",
    "                orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "                print(f\"Inserted {len(orders_df)} new transactions.\")\n",
    "            else:\n",
    "                print(\"No new transactions to insert.\")\n",
    "insert_into_orders_db(\"transactions.csv\", engine)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "75591328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update csv (read, transform, write)\n",
    "def update_csv(file, column_name, column_value, target_column, new_data):\n",
    "    # Read before update\n",
    "    before_update_df = read_csv_into_dfs(file)\n",
    "\n",
    "    # Apply update\n",
    "    after_update_df = before_update_df.copy()\n",
    "    after_update_df.loc[after_update_df[column_name] == column_value, target_column] = new_data\n",
    "    \n",
    "    # Save back to CSV\n",
    "    after_update_df.to_csv(file, index=False, header=True)\n",
    "\n",
    "    # Sync changes to DB (merge old vs new)\n",
    "    merge_transactions(engine, before_update_df, after_update_df)\n",
    "\n",
    "    \n",
    "update_csv(\"transactions.csv\",\"transaction_id\",12511,\"branch\",\"alex\") \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4101c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transactions(engine, old_df, new_df):\n",
    "    # Compare new vs old on transaction_id\n",
    "    merged = pd.merge(new_df, old_df, \n",
    "                      on=\"transaction_id\", how=\"left\", suffixes=(\"_new\", \"_old\"))\n",
    "\n",
    "    # ---- Insert new transactions ----\n",
    "    new_records = merged[merged[\"customer_id_old\"].isna()]\n",
    "    if not new_records.empty:\n",
    "        orders_new = merge_orders(new_records[columns_names], customers_df, products_df)\n",
    "        orders_new.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    # ---- Update old transactions ----\n",
    "    old_changed = merged[~merged[\"customer_id_old\"].isna()]\n",
    "    for _, row in old_changed.iterrows():\n",
    "        # Check if any column value actually changed\n",
    "        if (\n",
    "            row[\"customer_id_new\"] != row[\"customer_id_old\"] or\n",
    "            row[\"product_id_new\"] != row[\"product_id_old\"] or\n",
    "            row[\"quantity_new\"]   != row[\"quantity_old\"] or\n",
    "            row[\"transaction_date_new\"] != row[\"transaction_date_old\"] or\n",
    "            row[\"branch_new\"] != row[\"branch_old\"]\n",
    "        ):\n",
    "            update_sql = f\"\"\"\n",
    "            UPDATE TOrders\n",
    "            SET product_name = '{products_df.loc[products_df['id']==row['product_id_new'],'pname'].values[0]}',\n",
    "                amount = {row['quantity_new']} * (\n",
    "                    SELECT price FROM Tproducts WHERE id = {row['product_id_new']}\n",
    "                ),\n",
    "                customer_full_name = (\n",
    "                    SELECT first_name + ' ' + last_name FROM Tcustomers WHERE id = {row['customer_id_new']}\n",
    "                ),\n",
    "                transaction_date = '{row['transaction_date_new']}',\n",
    "                branch = '{row['branch_new']}'\n",
    "            WHERE transaction_id = {row['transaction_id']};\n",
    "            \"\"\"\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(text(update_sql))\n",
    "    tdf = read_csv_into_dfs(\"transactions.csv\")\n",
    "    cdf = read_csv_into_dfs(\"Customers.csv\")\n",
    "    pdf = pd.read_json(\"products.json\")\n",
    "    merge_orders(tdf,cdf,pdf)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aed29e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_product_price(engine, product_id, new_price):\n",
    "    update_sql = f\"\"\"\n",
    "    UPDATE TOrders\n",
    "    SET amount = {new_price} * (\n",
    "        SELECT quantity \n",
    "        FROM transactions t\n",
    "        WHERE t.transaction_id = TOrders.transaction_id\n",
    "          AND t.product_id = {product_id}\n",
    "    )\n",
    "    WHERE transaction_id IN (\n",
    "        SELECT transaction_id \n",
    "        FROM transactions \n",
    "        WHERE product_id = {product_id}\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(update_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas faker sqlalchemy pyodbc \n",
    "from sqlalchemy import inspect, text\n",
    "import random\n",
    "import shutil, os\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "products = [\n",
    "    { \"id\": 1, \"pname\": \"Laptop\", \"price\": 12000 },\n",
    "    { \"id\": 2, \"pname\": \"Mouse\", \"price\": 250 },\n",
    "    { \"id\": 3, \"pname\": \"Keyboard\", \"price\": 500 },\n",
    "    { \"id\": 4, \"pname\": \"Monitor\", \"price\": 3000 },\n",
    "    { \"id\": 5, \"pname\": \"Headphones\", \"price\": 800 },\n",
    "    { \"id\": 6, \"pname\": \"Printer\", \"price\": 4500 },\n",
    "    { \"id\": 7, \"pname\": \"USB Flash Drive\", \"price\": 150 }\n",
    "]\n",
    "with open(\"products.json\", \"w\") as file:\n",
    "    json.dump(products, file, indent=4)\n",
    "\n",
    "def new_transaction_data_generator(number_of_records,max_id):\n",
    "    return [\n",
    "            [\n",
    "                i,\n",
    "                random.randint(1,2001),\n",
    "                random.randint(1, 7),\n",
    "                random.randint(1, 10),\n",
    "                date.today(),\n",
    "                random.choice(['cairo', 'alex', 'tanta'])\n",
    "            ]\n",
    "            for i in range(max_id+1, max_id + number_of_records + 1)\n",
    "        ]    \n",
    "\n",
    "columns_names = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\",\"transaction_date\", \"branch\"]\n",
    "\n",
    "def create_transactions_dfs(num_of_files, number_of_records):\n",
    "    base_date = fake.date_between(\"-15d\",\"-4d\")\n",
    "    file_counter = 1\n",
    "    current_id = 0\n",
    "    \n",
    "    for i in range(num_of_files):\n",
    "        current_date = base_date + timedelta(days=i // 3)\n",
    "        \n",
    "        transaction_df = pd.DataFrame(\n",
    "            columns=columns_names,\n",
    "            data= new_transaction_data_generator(number_of_records,current_id)\n",
    "        )\n",
    "        \n",
    "        transaction_df.to_csv(f\"transactions{file_counter}.csv\", index=False)\n",
    "        file_counter += 1\n",
    "        current_id += number_of_records\n",
    "\n",
    "create_transactions_dfs(num_of_files=12, number_of_records=1000)\n",
    "\n",
    "def get_max_id(file):\n",
    "    if not os.path.exists(file):\n",
    "        return 0\n",
    "    existing_df = pd.read_csv(file)\n",
    "    return len(existing_df)\n",
    "\n",
    "main_file = \"transactions.csv\"\n",
    "def concat_files(sec_file):\n",
    "    # If the main file does not exist, just copy secondary to main\n",
    "    if not os.path.exists(main_file):\n",
    "        shutil.copy(sec_file, main_file)\n",
    "        return\n",
    "    \n",
    "    # Read \n",
    "    main_df = pd.read_csv(main_file)\n",
    "    # skip el line bta3 el header w ent bt read\n",
    "    sec_df = pd.read_csv(sec_file,skiprows=1,names=columns_names)\n",
    "\n",
    "    # Append directly (IDs already unique)\n",
    "    df_final = pd.concat([main_df, sec_df], ignore_index=True)\n",
    "    df_final.to_csv(main_file, index=False)\n",
    "\n",
    "for i in range(1, 13):\n",
    "    sec_file = f\"transactions{i}.csv\"\n",
    "    concat_files(sec_file)\n",
    "\n",
    "def create_customer_records(num_of_customers):\n",
    "    Ccolumn_names=['id','first_name','last_name','Caddress']\n",
    "    customers_df = pd.DataFrame(columns=Ccolumn_names,\n",
    "                                data =[[i,\n",
    "                                fake.first_name(),\n",
    "                                fake.last_name() ,\n",
    "                                fake.address().replace('\\n','')]for i in range(1, num_of_customers+1)] )    \n",
    "    \n",
    "    customers_df.to_csv('Customers.csv',index=False)\n",
    "\n",
    "transaction_df = pd.read_csv('transactions.csv')\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "products_df = pd.read_json('products.json')\n",
    "orders_df = pd.read_csv('Orders.csv')\n",
    "\n",
    "def merge_orders(transaction_df,customers_df,products_df):\n",
    "    merged_df = pd.merge(\n",
    "        transaction_df, customers_df,\n",
    "        left_on='customer_id', right_on='id',\n",
    "        how='inner', suffixes=('', '_cust')\n",
    "    )\n",
    "\n",
    "    \n",
    "    orders_df = pd.merge(\n",
    "        merged_df, products_df,\n",
    "        left_on='product_id', right_on='id',\n",
    "        how='inner', suffixes=('', '_prod')\n",
    "    )\n",
    "\n",
    "    orders_df['amount'] = (orders_df['price'] * orders_df['quantity']).round().astype('int64')\n",
    "    orders_df['customer_full_name'] = orders_df['first_name'] + ' ' + orders_df['last_name']\n",
    "\n",
    "    orders_df = orders_df[\n",
    "        ['transaction_id', 'pname', 'amount', 'customer_full_name', 'transaction_date', 'branch']\n",
    "    ].rename(columns={'pname': 'product_name'})\n",
    "\n",
    "    orders_df.to_csv('Orders.csv', index=False)\n",
    "\n",
    "    return orders_df\n",
    "\n",
    "def read_csv_into_dfs(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://@DESKTOP-1UV6SC4\\\\SQLEXPRESS/ETLTask\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes\"\n",
    "    \"&Encrypt=no\"\n",
    ")\n",
    "\n",
    "def load_into_ssms(engine):\n",
    "    orders_df.to_sql(\"TOrders\" , con=engine , if_exists=\"append\" , index= False)\n",
    "    customers_df.to_sql(\"Tcustomers\" , con=engine , if_exists=\"append\" , index= False)  \n",
    "\n",
    "def add_transactions_toCSV(number_of_records, file_path=\"transactions.csv\"):\n",
    "    columns_names = [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\",\"transaction_date\", \"branch\"]\n",
    "    if os.path.exists(file_path):\n",
    "        existing_df = pd.read_csv(file_path, usecols=[\"transaction_id\"])\n",
    "        max_id = existing_df[\"transaction_id\"].max()\n",
    "        write_header = False\n",
    "        \n",
    "    else:\n",
    "        max_id = 0\n",
    "        write_header = True\n",
    "\n",
    "    new_data = new_transaction_data_generator(number_of_records,max_id)\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=columns_names)\n",
    "\n",
    "    new_df.to_csv(file_path, index=False, header=write_header, mode='a')  \n",
    "\n",
    "\n",
    "def insert_into_orders_db(file, engine):\n",
    "    transaction_df = read_csv_into_dfs(file)\n",
    "\n",
    "    inspector = inspect(engine)\n",
    "    table_exists = inspector.has_table(\"TOrders\")\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create table\n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE TOrders (\n",
    "            transaction_id INT PRIMARY KEY,\n",
    "            product_name NVARCHAR(100),\n",
    "            amount INT,\n",
    "            customer_full_name NVARCHAR(200),\n",
    "            transaction_date DATE,\n",
    "            branch NVARCHAR(100)\n",
    "        );\n",
    "        \"\"\"\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "\n",
    "        # Merge & insert all transactions\n",
    "        orders_df = merge_orders(transaction_df, customers_df, products_df)\n",
    "        orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    else:\n",
    "        with engine.connect() as conn:\n",
    "            row_count = conn.execute(text(\"SELECT COUNT(*) FROM TOrders\")).scalar()\n",
    "\n",
    "        if row_count == 0:\n",
    "            orders_df = merge_orders(transaction_df, customers_df, products_df)\n",
    "            orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "        else:\n",
    "            # Get max transaction_id already in DB\n",
    "            max_id_df = pd.read_sql(\"SELECT MAX(transaction_id) AS max_id FROM TOrders\", con=engine)\n",
    "            max_id = max_id_df.iloc[0, 0]\n",
    "\n",
    "            # Filter only new transactions\n",
    "            transaction_new = transaction_df[transaction_df[\"transaction_id\"] > max_id]\n",
    "\n",
    "            if not transaction_new.empty:\n",
    "                orders_df = merge_orders(transaction_new, customers_df, products_df)\n",
    "                orders_df.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "                print(f\"Inserted {len(orders_df)} new transactions.\")\n",
    "            else:\n",
    "                print(\"No new transactions to insert.\")\n",
    "\n",
    "#update csv (read, transform, write)\n",
    "def update_csv(file, column_name, column_value, target_column, new_data):\n",
    "    # Read before update\n",
    "    before_update_df = read_csv_into_dfs(file)\n",
    "\n",
    "    # Apply update\n",
    "    after_update_df = before_update_df.copy()\n",
    "    after_update_df.loc[after_update_df[column_name] == column_value, target_column] = new_data\n",
    "    \n",
    "    # Save back to CSV\n",
    "    after_update_df.to_csv(file, index=False, header=True)\n",
    "\n",
    "    # Sync changes to DB (merge old vs new)\n",
    "    merge_transactions(engine, before_update_df, after_update_df)        \n",
    "\n",
    "def merge_transactions(engine, old_df, new_df):\n",
    "    # Compare new vs old on transaction_id\n",
    "    merged = pd.merge(new_df, old_df, \n",
    "                      on=\"transaction_id\", how=\"left\", suffixes=(\"_new\", \"_old\"))\n",
    "\n",
    "    # ---- Insert new transactions ----\n",
    "    new_records = merged[merged[\"customer_id_old\"].isna()]\n",
    "    if not new_records.empty:\n",
    "        orders_new = merge_orders(new_records[columns_names], customers_df, products_df)\n",
    "        orders_new.to_sql(\"TOrders\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "    # ---- Update old transactions ----\n",
    "    old_changed = merged[~merged[\"customer_id_old\"].isna()]\n",
    "    for _, row in old_changed.iterrows():\n",
    "        # Check if any column value actually changed\n",
    "        if (\n",
    "            row[\"customer_id_new\"] != row[\"customer_id_old\"] or\n",
    "            row[\"product_id_new\"] != row[\"product_id_old\"] or\n",
    "            row[\"quantity_new\"]   != row[\"quantity_old\"] or\n",
    "            row[\"transaction_date_new\"] != row[\"transaction_date_old\"] or\n",
    "            row[\"branch_new\"] != row[\"branch_old\"]\n",
    "        ):\n",
    "            update_sql = f\"\"\"\n",
    "            UPDATE TOrders\n",
    "            SET product_name = '{products_df.loc[products_df['id']==row['product_id_new'],'pname'].values[0]}',\n",
    "                amount = {row['quantity_new']} * (\n",
    "                    SELECT price FROM Tproducts WHERE id = {row['product_id_new']}\n",
    "                ),\n",
    "                customer_full_name = (\n",
    "                    SELECT first_name + ' ' + last_name FROM Tcustomers WHERE id = {row['customer_id_new']}\n",
    "                ),\n",
    "                transaction_date = '{row['transaction_date_new']}',\n",
    "                branch = '{row['branch_new']}'\n",
    "            WHERE transaction_id = {row['transaction_id']};\n",
    "            \"\"\"\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(text(update_sql))\n",
    "    tdf = read_csv_into_dfs(\"transactions.csv\")\n",
    "    cdf = read_csv_into_dfs(\"Customers.csv\")\n",
    "    pdf = pd.read_json(\"products.json\")\n",
    "    merge_orders(tdf,cdf,pdf)            \n",
    "\n",
    "def update_product_price(engine, product_id, new_price):\n",
    "    update_sql = f\"\"\"\n",
    "    UPDATE TOrders\n",
    "    SET amount = {new_price} * (\n",
    "        SELECT quantity \n",
    "        FROM transactions t\n",
    "        WHERE t.transaction_id = TOrders.transaction_id\n",
    "          AND t.product_id = {product_id}\n",
    "    )\n",
    "    WHERE transaction_id IN (\n",
    "        SELECT transaction_id \n",
    "        FROM transactions \n",
    "        WHERE product_id = {product_id}\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(update_sql))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2058e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
